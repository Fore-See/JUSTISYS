{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文字探勘實作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那我\n",
      "我們\n",
      "們酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence='那我們酸民婉君也可以報名嗎'\n",
    "\n",
    "sentence  = input_sentence.decode('utf-8')\n",
    "\n",
    "for i in range(0, len(sentence) - 2 + 1):\n",
    "    print sentence[i:i+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那我們\n",
      "我們酸\n",
      "們酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(sentence) -3 + 1):\n",
    "    print sentence[i:i+3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 n-gram 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    word_dic = {}\n",
    "    sentence  = input_sentence.decode('utf-8')\n",
    "    for i in range(0, len(sentence) - n + 1):\n",
    "        if sentence[i:i+n] not in word_dic:\n",
    "            word_dic[sentence[i:i+n]] = 1\n",
    "        else:\n",
    "            word_dic[sentence[i:i+n]] = word_dic[sentence[i:i+n]] + 1\n",
    "    return word_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試 bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "婉君 報名 可以 名嗎 君也 也可 民婉 酸民 以報\n"
     ]
    }
   ],
   "source": [
    "for word in ngram('酸民婉君也可以報名嗎'):\n",
    "    print word,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試 trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酸民婉\n",
      "君也可\n",
      "名嗎?\n",
      "可以報\n",
      "也可以\n",
      "們酸民\n",
      "以報名\n",
      "民婉君\n",
      "報名嗎\n",
      "那我們\n",
      "婉君也\n",
      "我們酸\n"
     ]
    }
   ],
   "source": [
    "for word in ngram('那我們酸民婉君也可以報名嗎?' ,n=3):\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除標點符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '123?'\n",
    "skip_word = '.?'\n",
    "len([word for word in sentence if word in skip_word]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skip_dic = '?.,、　「！】？：《」》～【'.decode('utf-8')\n",
    "def ngram(input_sentence, n = 2, skip_word= skip_dic):\n",
    "    word_dic = {}\n",
    "    sentence  = input_sentence.decode('utf-8')\n",
    "    for i in range(0, len(sentence) - n + 1):\n",
    "        invalid = len([word for word in sentence[i:i+n] if word in skip_word])\n",
    "        if sentence[i:i+n] not in word_dic and invalid ==0:\n",
    "            word_dic[sentence[i:i+n]] = 1\n",
    "        elif invalid ==0:\n",
    "            word_dic[sentence[i:i+n]] = word_dic[sentence[i:i+n]] + 1\n",
    "    return word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們\n",
      "婉君\n",
      "報名\n",
      "們酸\n",
      "可以\n",
      "名嗎\n",
      "君也\n",
      "也可\n",
      "那我\n",
      "酸民\n",
      "以報\n"
     ]
    }
   ],
   "source": [
    "for word in ngram('那我們酸民、婉君也可以報名嗎?'):\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: news_entry",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-62e3bbc136ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'news.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'select * from news_entry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mallNews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: news_entry"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect('news.db') \n",
    "cur = db.cursor()\n",
    "cur.execute('select * from news_entry')\n",
    "allNews = cur.fetchall()\n",
    "dic = {}\n",
    "for rec in allNews:\n",
    "    ngram_dic = ngram(rec[4].encode('utf-8'))\n",
    "    for ele in ngram_dic:\n",
    "        if ele not in dic:\n",
    "            dic[ele] = ngram_dic[ele]\n",
    "        else:\n",
    "            dic[ele] = dic[ele] + ngram_dic[ele]\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-1b89c3395de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwords_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_freq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dic' is not defined"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "words_freq = sorted(dic.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "for word in words_freq:\n",
    "    if word[1] >= 2:\n",
    "        print word[0], word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 長詞優先演算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何依標點符號切開字詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3年前就讀高雄應用大學二年級的陳顯佳\n",
      "為閃避跨越雙黃線來車\n",
      "撞上路邊\n",
      "摔斷頸椎\n",
      "頸部以下全癱\n",
      "嘴被插管\n",
      "父母每天去看他\n",
      "他都意識清醒\n",
      "眨眼流淚\n",
      "17天後他因頸椎受損抑制呼吸\n",
      "醫師用盡強力針與電擊都無效\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "delimiter = \"，|。|、\"  \n",
    "text = '3年前就讀高雄應用大學二年級的陳顯佳，為閃避跨越雙黃線來車，撞上路邊、摔斷頸椎，頸部以下全癱，嘴被插管，父母每天去看他，他都意識清醒、眨眼流淚，17天後他因頸椎受損抑制呼吸，醫師用盡強力針與電擊都無效，'\n",
    "for i in re.split(delimiter, text):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試切除效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: news_entry",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-6e0fbfc9d3cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'news.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'select * from news_entry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mallNews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msentenceAry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: news_entry"
     ]
    }
   ],
   "source": [
    "def splitSentense(text, delimiter):\n",
    "    return re.split(delimiter, text)\n",
    "    \n",
    "delimiter = \"，|。|、|（|）|／|《|》|】|【|「|」|；|：\".decode('utf-8')  \n",
    "db = sqlite3.connect('news.db') \n",
    "cur = db.cursor()\n",
    "cur.execute('select * from news_entry')\n",
    "allNews = cur.fetchall()\n",
    "sentenceAry = []\n",
    "for rec in allNews:\n",
    "    text = rec[5]\n",
    "    sentenceAry += splitSentense(text,delimiter)\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentenceAry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8d1d99589f63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentenceAry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentenceAry' is not defined"
     ]
    }
   ],
   "source": [
    "for sentence in sentenceAry[0:8]:\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentenceAry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1b5e486116ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtextAry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextAry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtextAry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0msentenceAry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mremoveKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceAry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'民進黨'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'後天'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentenceAry' is not defined"
     ]
    }
   ],
   "source": [
    "def removeKey(text, keyword):\n",
    "    textAry= text\n",
    "    for key in keyword:\n",
    "        textAry = ''.join(textAry.split(key.decode('utf-8')))\n",
    "    return textAry\n",
    "print sentenceAry[0]\n",
    "print removeKey(sentenceAry[0], ['民進黨', '後天'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = '民進黨後天將公布'.decode('utf-8')\n",
    "removeKey(a, ['民進黨', '後天'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改ngram 處理函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    word_dic = {}\n",
    "    sentence  = input_sentence\n",
    "    for i in range(0, len(sentence) - n + 1):        \n",
    "        if sentence[i:i+n] not in word_dic:\n",
    "            word_dic[sentence[i:i+n]] = 1\n",
    "        else:\n",
    "            word_dic[sentence[i:i+n]] = word_dic[sentence[i:i+n]] + 1\n",
    "    return word_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依長詞優先產生關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords=[]        \n",
    "ret_terms={}\n",
    "words_freq    = []\n",
    "for term_length in range(4,1,-1):\n",
    "    word_dic = {}\n",
    "    for sentence in sentenceAry:\n",
    "        text_list = removeKey(sentence,keywords)        \n",
    "        ngram_words = ngram(text_list,term_length) \n",
    "        for word in ngram_words:\n",
    "            if word not in word_dic:\n",
    "                word_dic[word] = 1\n",
    "            else:\n",
    "                word_dic[word] += ngram_words[word]   \n",
    "    for word in word_dic:\n",
    "        if word_dic[word] >= 5:\n",
    "            keywords.append(word.encode('utf-8'))            \n",
    "            ret_terms.update({word:word_dic[word]})\n",
    "\n",
    "sorted_terms = sorted(ret_terms.iteritems(),key=operator.itemgetter(1),reverse=True) \n",
    "for term in sorted_terms[0:30]:\n",
    "    print term[0], term[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "sentence = \"大巨蛋案對市府同仁下封口令？　柯P否認\"\n",
    "words = jieba.cut(sentence, cut_all=False)\n",
    "for word in words:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用jieba 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\", cut_all=True)\n",
    "print \"Full Mode:\", \"/ \".join(seg_list) \n",
    "\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\", cut_all=False)\n",
    "print \"Default Mode:\", \"/ \".join(seg_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用自訂辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.load_userdict(\"userdict.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新增或刪除字詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.add_word('柯P',100, 'nr')\n",
    "#jieba.del_word('柯P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表列詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\")\n",
    "for w in words:\n",
    "    print w.word, w.flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列出關鍵字所在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = jieba.tokenize(unicode(sentence, 'utf-8'))\n",
    "\n",
    "for tw in words:\n",
    "    print tw[0], tw[1], tw[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依詞性切出關鍵詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "tags = jieba.analyse.extract_tags(sentence, 1, allowPOS = ['nr'])\n",
    "print \",\".join(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列出斷詞位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = jieba.tokenize(unicode(sentence, 'utf-8'))\n",
    "\n",
    "for tw in words:\n",
    "    print tw[0], tw[1], tw[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表列關鍵詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "sentence = \"根據歐洲電商組織(Ecommerce Europe) 最新消息得知，\\\n",
    "該集團積極讓歐洲的包裹遞送服務與電商領域有更緊密結合。如此一來，\\\n",
    "便可解決電商物流的障礙。此組織總共結合歐洲 16 國的電商協會，\\\n",
    "目標整合歐洲的包裹物流市場，讓商品跨境更為順暢\"\n",
    "tags = jieba.analyse.extract_tags(sentence)\n",
    "print '/'.join(tags)\n",
    "\n",
    "jieba.analyse.set_stop_words(\"stopword.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成文字雲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import jieba\n",
    "section = requests.get('http://www.appledaily.com.tw/realtimenews/section/new/1')\n",
    "soup = BeautifulSoup(section.text)\n",
    "domain = 'http://www.appledaily.com.tw'\n",
    "worddic = {}\n",
    "for news in soup.select('.rtddt'):    \n",
    "    main = news.select('h1')[0].text\n",
    "    m = re.match('(.*)\\((\\d+)\\)',main)\n",
    "    if m:\n",
    "        title = m.group(1)\n",
    "        popularity = int(m.group(2))\n",
    "    for wd in jieba.cut(title):\n",
    "        if wd not in worddic:\n",
    "            worddic[wd] = 1\n",
    "        else:\n",
    "            worddic[wd] = worddic[wd] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytagcloud import create_tag_image, make_tags\n",
    "from IPython.display import Image\n",
    "from operator import itemgetter\n",
    "swd = sorted(worddic.iteritems(), key=itemgetter(1), reverse=True)\n",
    "swd = [w for w in swd[1:50] if w[0] not in '【】...'.decode('utf-8')]\n",
    "\n",
    "tags = make_tags(swd, maxsize=120)\n",
    "create_tag_image(tags, 'wc.png', size=(600,400), fontname='SimHei')\n",
    "Image(filename='wc.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import jieba\n",
    "section = requests.get('http://www.appledaily.com.tw/realtimenews/section/new/1')\n",
    "soup = BeautifulSoup(section.text)\n",
    "domain = 'http://www.appledaily.com.tw'\n",
    "worddic = {}\n",
    "for news in soup.select('.rtddt'):    \n",
    "    main = news.select('h1')[0].text\n",
    "    m = re.match('(.*)\\((\\d+)\\)',main)\n",
    "    if m:\n",
    "        title = m.group(1)\n",
    "        popularity = int(m.group(2))\n",
    "    for wd in jieba.analyse.extract_tags(title):\n",
    "        if wd not in worddic:\n",
    "            worddic[wd] = 1\n",
    "        else:\n",
    "            worddic[wd] = worddic[wd] + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用pytagcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytagcloud import create_tag_image, make_tags\n",
    "from IPython.display import Image\n",
    "from operator import itemgetter\n",
    "swd = sorted(worddic.iteritems(), key=itemgetter(1), reverse=True)\n",
    "swd = [w for w in swd[1:50]]\n",
    "\n",
    "tags = make_tags(swd, maxsize=120)\n",
    "create_tag_image(tags, 'wc.png', size=(600,400), fontname='SimHei')\n",
    "Image(filename='wc.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytagcloud import create_tag_image, make_tags\n",
    "from IPython.display import Image\n",
    "from operator import itemgetter\n",
    "swd = sorted(worddic.iteritems(), key=itemgetter(1), reverse=True)\n",
    "swd = [w for w in swd[1:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 tagcanvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML \n",
    "\n",
    "page= '''\n",
    "  <head>\n",
    "    <title>Word Cloud</title>\n",
    "    <script src=\"tagcanvas.min.js\" type=\"text/javascript\"></script>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Word Cloud</h1>\n",
    "    <div id=\"myCanvasContainer\">\n",
    "      <canvas width=\"300\" height=\"300\" id=\"myCanvas\">\n",
    "        <p>Anything in here will be replaced on browsers that support the canvas element</p>\n",
    "      </canvas>\n",
    "    </div>\n",
    "    <div id=\"tags\"><ul id=\"weightTags\">%s</ul></div>\n",
    "  </body>\n",
    "'''\n",
    "\n",
    "js = '''\n",
    "TagCanvas.Start('myCanvas','weightTags', {\n",
    " textFont: null,\n",
    " textColour: null,\n",
    " weight: true,\n",
    " weightMode: 'both',\n",
    " textFont: 'Impact,\"Arial Black\",sans-serif',\n",
    " /* more options */\n",
    " \n",
    "});\n",
    "$('#tags').css('display','None');\n",
    "'''\n",
    "\n",
    "import IPython\n",
    "from IPython.core.display import display_html, display_javascript, Javascript\n",
    "\n",
    "js_libs = ['tagcanvas.min.js']\n",
    "li_str = ''\n",
    "for li in swd:\n",
    "    li_str += '<li><a data-weight=\"10\" style=\"font-size: %dex\">%s</a></li>'%(li[1] , li[0])\n",
    "    \n",
    "display_html(IPython.core.display.HTML(data=page%(li_str)))\n",
    "display_javascript( Javascript(data=js, lib= js_libs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "ary = ['【更新】柯P：洪智坤洩漏公文案還沒看到公文　今處理',\n",
    "'留洪智坤 柯：殘障求職不易',\n",
    "'人事處議處洪智坤　柯P：不清楚議處結果']\n",
    "corpus = []\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))\n",
    "\n",
    "jieba.add_word('洪智坤',100, 'nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易\n",
      "人事\n",
      "今處理\n",
      "公文\n",
      "更新\n",
      "案還\n",
      "殘障\n",
      "求職\n",
      "洩漏\n",
      "洪智坤\n",
      "清楚\n",
      "留洪智坤\n",
      "看到\n",
      "結果\n",
      "處議\n",
      "議處\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() \n",
    "for w in word:\n",
    "    print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4347)\t0.254443683779\n",
      "  (0, 5686)\t0.172982331747\n",
      "  (0, 6359)\t0.146961402446\n",
      "  (0, 4296)\t0.239772224518\n",
      "  (0, 3304)\t0.254443683779\n",
      "  (0, 4534)\t0.147901306479\n",
      "  (0, 7461)\t0.181535887301\n",
      "  (0, 641)\t0.22128837183\n",
      "  (0, 109)\t0.196207346562\n",
      "  (0, 129)\t0.204281633244\n",
      "  (0, 471)\t0.254443683779\n",
      "  (0, 1192)\t0.254443683779\n",
      "  (0, 6986)\t0.254443683779\n",
      "  (0, 3805)\t0.254443683779\n",
      "  (0, 4724)\t0.209113369312\n",
      "  (0, 4010)\t0.254443683779\n",
      "  (0, 3717)\t0.239772224518\n",
      "  (0, 2308)\t0.214691199251\n",
      "  (0, 4194)\t0.239772224518\n",
      "  (0, 922)\t0.22128837183\n",
      "  (1, 4828)\t0.221318941845\n",
      "  (1, 4913)\t0.275664561512\n",
      "  (1, 6190)\t0.216701602095\n",
      "  (1, 4352)\t0.248491751679\n",
      "  (1, 1839)\t0.25976948672\n",
      "  :\t:\n",
      "  (830, 641)\t0.239816497909\n",
      "  (831, 3323)\t0.235294616154\n",
      "  (831, 105)\t0.235294616154\n",
      "  (831, 266)\t0.235294616154\n",
      "  (831, 180)\t0.235294616154\n",
      "  (831, 7021)\t0.235294616154\n",
      "  (831, 70)\t0.235294616154\n",
      "  (831, 2387)\t0.212101152966\n",
      "  (831, 211)\t0.221727309927\n",
      "  (831, 169)\t0.212101152966\n",
      "  (831, 167)\t0.204634525548\n",
      "  (831, 32)\t0.221727309927\n",
      "  (831, 68)\t0.188907689778\n",
      "  (831, 7668)\t0.188907689778\n",
      "  (831, 1254)\t0.178251879444\n",
      "  (831, 5785)\t0.212101152966\n",
      "  (831, 5586)\t0.198533846739\n",
      "  (831, 3002)\t0.152146920364\n",
      "  (831, 3537)\t0.149468603773\n",
      "  (831, 2942)\t0.17266206696\n",
      "  (831, 2978)\t0.136770465746\n",
      "  (831, 194)\t0.204634525548\n",
      "  (831, 172)\t0.204634525548\n",
      "  (831, 4312)\t0.198533846739\n",
      "  (831, 6946)\t0.181441062361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray()    \n",
    "# print weight\n",
    "print tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.          0.07912735]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[0], tfidf).flatten()\n",
    "print cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列出最相關文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【更新】柯P：洪智坤洩漏公文案還沒看到公文　今處理\n",
      "人事處議處洪智坤　柯P：不清楚議處結果\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-3:-1]\n",
    "related_docs_indices\n",
    "for index in related_docs_indices:\n",
    "    print ary[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析6/17 新聞內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree\n",
    "import jieba.analyse\n",
    "\n",
    "f = open('1434435247.xml', 'r')\n",
    "events=ElementTree.fromstring(f.read())\n",
    "f.close()\n",
    "corpus = []\n",
    "ary= []\n",
    "for elem in events.findall('./channel/item'):\n",
    "    guid = elem.find('guid').text\n",
    "    title = elem.find('title').text\n",
    "    description = elem.find('description').text\n",
    "    pubDate = elem.find('pubDate').text\n",
    "    source = elem.find('source').text\n",
    "    ary.append(title)\n",
    "    corpus.append(' '.join(jieba.analyse.extract_tags(description, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray() \n",
    "print weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.02416427  0.          0.          0.02305006  0.\n",
      "  0.02051457  0.          0.          0.          0.02170186  0.          0.\n",
      "  0.          0.          0.03182717  0.0234633   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.0310064\n",
      "  0.          0.          0.0475188   0.          0.          0.          0.\n",
      "  0.          0.          0.0228061   0.          0.          0.02111917\n",
      "  0.          0.          0.          0.          0.0247604   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.02222974\n",
      "  0.          0.02313953  0.          0.          0.          0.\n",
      "  0.02406637  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.07723495  0.          0.          0.          0.0211354   0.\n",
      "  0.02310812  0.          0.          0.          0.02549601  0.          0.\n",
      "  0.          0.          0.          0.          0.04767037  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.0475188   0.\n",
      "  0.          0.          0.          0.          0.          0.0222501   0.\n",
      "  0.          0.04149069  0.          0.          0.          0.          0.\n",
      "  0.04662311  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02389799  0.          0.          0.          0.          0.          0.\n",
      "  0.02197937  0.          0.02664673  0.          0.          0.\n",
      "  0.02395035  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.02072397  0.          0.          0.          0.04587164\n",
      "  0.          0.          0.          0.          0.02173648  0.02992429\n",
      "  0.          0.          0.05342     0.          0.          0.          0.\n",
      "  0.          0.          0.09447222  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.03540916  0.02798137  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.02248769\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.03095706  0.02649427  0.          0.03152859  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.02173531  0.          0.\n",
      "  0.          0.          0.          0.03265195  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.02211279  0.          0.02126164  0.          0.          0.\n",
      "  0.          0.04681661  0.04822059  0.04649881  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.0234903   0.\n",
      "  0.          0.          0.          0.          0.          0.02122594\n",
      "  0.          0.          0.02478965  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.0293733\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.03676196  0.03453384  0.          0.          0.02623797\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.02974823\n",
      "  0.          0.          0.          0.04814355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.05949502  0.          0.          0.02124756  0.          0.          0.\n",
      "  0.          0.          0.          0.02369862  0.          0.          0.\n",
      "  0.          0.          0.02722404  0.          0.          0.03083422\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.02041741  0.          0.02167495\n",
      "  0.          0.02048823  0.          0.          0.          0.          0.\n",
      "  0.0226167   0.          0.          0.          0.          0.02805635\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02532374  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.03352668  0.          0.          0.\n",
      "  0.03213688  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.0216489   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02291584  0.03471154  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.032936    0.          0.\n",
      "  0.03020354  0.          0.          0.          0.          0.02461088\n",
      "  0.04577487  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.02189821  0.          0.          0.          0.\n",
      "  0.02671312  0.02481555  0.03352756  0.          0.          0.          0.\n",
      "  0.          0.17086647  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.05439043\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.03899796  0.          0.          0.          0.\n",
      "  0.          0.05699128  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.03691935  0.          0.          0.          0.          0.02082654\n",
      "  0.02233051  0.          0.04146623  0.          0.          0.02650291\n",
      "  0.          0.04504502  0.          0.          0.          0.04863693\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.02627289  0.          0.          0.          0.          0.          0.\n",
      "  0.02237662  0.          0.05017346  0.          0.          0.          0.\n",
      "  0.0205236   0.          0.          0.          0.          0.          0.0423166\n",
      "  0.          0.          0.          0.          0.06458536  0.\n",
      "  0.04567599  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.03963894  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02427506  0.          0.02237985  0.          0.          0.          0.\n",
      "  0.02247395  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.02221916\n",
      "  0.          0.          0.09963168  0.02863453  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.03287744  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.02375695  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.04822495  0.02287846  0.          0.          0.          0.\n",
      "  0.          0.          0.02249646  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.0324561   0.          0.\n",
      "  0.02954989  0.03404823  0.          0.03283749  0.          0.\n",
      "  0.03706324  0.          0.          0.          0.          0.\n",
      "  0.02291261  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.020557    0.\n",
      "  0.          0.          0.          0.          0.          0.03019332\n",
      "  0.          0.          0.02259136  0.          0.          0.\n",
      "  0.02099059  0.03149475  0.          0.          0.          0.          0.\n",
      "  0.          0.07677494  0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[0], tfidf).flatten()\n",
    "# print cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "n_cosine_similarities = linear_kernel(tfidf[0:10], tfidf[0:10])\n",
    "#print cosine_similarities\n",
    "print n_cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 HeatMap 視覺化相似程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-25f2b9dd6a31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# pip install seaborn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cosine_similarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coolwarm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# pip install seaborn\n",
    "sns.heatmap(n_cosine_similarities, annot=True, center=0, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-10:-1]\n",
    "print related_docs_indices\n",
    "for index in related_docs_indices:\n",
    "    print ary[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "n_cosine_similarities = linear_kernel(tfidf, tfidf)\n",
    "#print cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用PCA 降維資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.02416427  0.        ]\n",
      " [ 0.          1.          0.          0.          0.          0.14021852\n",
      "   0.038673    0.          0.01747556  0.        ]\n",
      " [ 0.          0.          1.          0.          0.          0.          0.\n",
      "   0.          0.04299854  0.        ]\n",
      " [ 0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          1.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.14021852  0.          0.          0.          1.\n",
      "   0.0706901   0.          0.01712003  0.        ]\n",
      " [ 0.          0.038673    0.          0.          0.          0.0706901\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.        ]\n",
      " [ 0.02416427  0.01747556  0.04299854  0.          0.          0.01712003\n",
      "   0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.        ]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-15e2df05d71a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cosine_similarities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "c = cluster.KMeans(n_cosine_similarities,3)\n",
    "print c.n_clusters\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "a = pca.fit(n_cosine_similarities)\n",
    "print a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(n_cosine_similarities)\n",
    "\n",
    "print pca_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(832, 7697)\n"
     ]
    }
   ],
   "source": [
    "print weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=4, n_init=10,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=0)\n"
     ]
    }
   ],
   "source": [
    "param = [{'model':cluster.KMeans(n_clusters=4),\n",
    "          'title':'KMeans'}]\n",
    "c = param[0]['model']\n",
    "k_data = c.fit_predict(weight)\n",
    "print weight\n",
    "print c\n",
    "# print k_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢視資料與Sum of Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(832,)\n",
      "808.880296082\n"
     ]
    }
   ],
   "source": [
    "print k_data.shape\n",
    "print c.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 3 3 3 3 3 0 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 1\n",
      " 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 3 3 0 3 3 1 3 3 3 3 3 3 3 3 0 3\n",
      " 3 0 3 1 1 1 3 3 3 0 3 3 3 3 0 0 3 3 3 3 3 3 3 3 2 3 3 3 3 0 3 3 0 0 3 3 3\n",
      " 3 3 0 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 0 2 3 0 3 3 3 1 1 1 1 3 1 1 0 3 3 3 3\n",
      " 3 0 0 3 3 3 3 3 0 3 3 0 3 3 3 3 3 2 3 3 2 2 3 2 3 0 3 3 3 3 3 3 2 3 3 3 3\n",
      " 3 0 3 3 2 3 3 3 3 3 0 3 3 3 0 3 0 3 3 3 3 3 1 1 3 1 1 1 3 3 1 1 1 1 3 1 1\n",
      " 1 1 1 3 1 1 3 3 3 3 3 2 3 0 2 3 3 3 0 0 3 3 2 3 3 3 0 3 3 3 3 2 3 0 3 2 3\n",
      " 3 3 0 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 2 3 0 3 3 3 3 3 3 3 2 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 3 0 0 3 2 2 0 3 3 3 3 3 3 3 2 3 0 0 3 3 3 3 3 3 3 3 3 0\n",
      " 3 3 3 3 3 2 3 3 3 0 3 3 3 0 3 3 3 2 3 2 0 3 3 3 3 3 3 0 3 3 3 3 3 3 1 3 1\n",
      " 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 2 0 3 3 3 3 2 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 1 3 1 1 3 3 1 0 3 0 3 3 3 3 3 3 3 3 3 3 0 3 3 3 1 1 3 1 1\n",
      " 1 3 3 3 3 3 3 2 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3\n",
      " 3 0 0 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 0 1 1 1 1 1 1\n",
      " 3 3 3 2 3 3 3 0 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 1 2 1 2 1 1 3 3 3 3 3 3 3 3 3 3 3 0 3 3 1 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 1 1 3 3 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 0 0 3\n",
      " 3 3 0 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print k_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化呈現分群結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xabcaa70>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFCCAYAAAAg3dP7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xabc3dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=1, nrows=1,figsize=(10,5))\n",
    "axes.scatter(pca_data[:,0], pca_data[:,1], c=['rgbyc'[i] for i in k_data ], s=200)\n",
    "axes.set_title(param[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "print vectorizer.get_feature_names()\n",
    "print(X.toarray().transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "posts = [open(os.path.join('toy/', f)).read() for f in os.listdir('toy')]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features)) \n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print new_post_vec\n",
    "print(new_post_vec.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "sorted(vectorizer.get_stop_words())[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")\n",
    "\n",
    "s.stem(\"imaging\")\n",
    "s.stem(\"image\")\n",
    "s.stem(\"imagination\")\n",
    "s.stem(\"imagine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
